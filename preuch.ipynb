{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Faster-RCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "\n",
        "# For training\n",
        "images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n",
        "boxes[:, :, 2:4] = boxes[:, :, 0:2] + boxes[:, :, 2:4]\n",
        "labels = torch.randint(1, 91, (4, 11))\n",
        "images = list(image for image in images)\n",
        "targets = []\n",
        "for i in range(len(images)):\n",
        "    d = {}\n",
        "    d['boxes'] = boxes[i]\n",
        "    d['labels'] = labels[i]\n",
        "    targets.append(d)\n",
        "output = model(images, targets)\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)\n",
        "\n",
        "# optionally, if you want to export the model to ONNX:\n",
        "torch.onnx.export(model, x, \"faster_rcnn.onnx\", opset_version = 11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG9Tomyzdsg2"
      },
      "source": [
        "# YOLOv5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzfz3Ob4dvHP",
        "outputId": "83c2ac2f-3371-4a48-d1a6-9fc38d79d1da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 ðŸš€ v7.0-108-g4db6757 Python-3.8.10 torch-1.13.1+cu116 CPU\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 24.4/225.8 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images\n",
        "# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
