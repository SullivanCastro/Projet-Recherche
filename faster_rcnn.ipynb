{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fe745e",
   "metadata": {},
   "source": [
    "# Dataset S-VED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7950fc21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sullivan/Projet-Recherche\n"
     ]
    }
   ],
   "source": [
    "%cd ~/hdd/Projet-Recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc6e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55f687",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d242436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVED_Compose(object):\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class SVED_ToTensor(object):\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class SVED_RandomHorizontalFlip(object):\n",
    "\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-1)\n",
    "            bbox = target[\"boxes\"]\n",
    "            # bbox: xmin, ymin, xmax, ymax\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffaee957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVED_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, files_dir, transforms=None):\n",
    "        self._transforms = transforms\n",
    "        self.files_dir = files_dir\n",
    "\n",
    "        # sorting the images for consistency\n",
    "        # To get images, the extension of the filename is checked to be jpg\n",
    "        self.data = pd.read_csv(\"SVED_RCNN.csv\", index_col=[0])\n",
    "        self.imgs = [image for image in sorted(os.listdir(files_dir))\n",
    "                        if image[-4:]=='.jpg']\n",
    "        self.img_mean = [0.485, 0.456, 0.406]\n",
    "        self.img_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "\n",
    "        # classes: 0 index is reserved for background\n",
    "        self.classes = {'_': 0, 'Initial': 1, 'Decoration': 2, 'ContentIllustration': 3, 'PrintersMark': 4}\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.data['label'].replace(self.classes, inplace=True)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # img_name = self.imgs[idx]\n",
    "        img_name = self.imgs[idx]\n",
    "        image_path = os.path.join(self.files_dir, img_name)\n",
    "\n",
    "        # annotation file\n",
    "        data_img = self.data[self.data['file'] == img_name]\n",
    "\n",
    "        # reading the images and converting them to correct size and color\n",
    "        img = cv2.imread(image_path)\n",
    "\n",
    "        # width and heights parameters\n",
    "        width, height = img.shape[:-1]\n",
    "\n",
    "        # Preprocessing of the image\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        img_rgb = img_rgb\n",
    "        img_res = cv2.resize(img_rgb, (width, height), cv2.INTER_AREA)\n",
    "        # diving by 255\n",
    "        img_res /= 255.0\n",
    "        img_res = ((img_res - self.img_mean) / self.img_std).astype(np.float32)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        # box coordinates for xml files are extracted and corrected for image size given\n",
    "        for index, row in data_img.iterrows():\n",
    "            xmin = row['x'] * width\n",
    "            xmax = (row['x'] + row['width']) * width\n",
    "            ymin = row['y'] * height\n",
    "            ymax = (row['y'] + row['height']) * height\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            label = row['label']\n",
    "            labels.append(label)\n",
    "\n",
    "        # convert boxes into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # getting the areas of the boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "\n",
    "        targets = {}\n",
    "        targets[\"boxes\"] = boxes\n",
    "        targets[\"labels\"] = labels\n",
    "        targets[\"area\"] = area\n",
    "        targets[\"iscrowd\"] = iscrowd\n",
    "        targets[\"image_id\"] = torch.tensor([idx])\n",
    "\n",
    "\n",
    "        if self._transforms:\n",
    "            img_res, targets = self._transforms(img_res, targets)\n",
    "\n",
    "        return img_res, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def collate_fn(batch):\n",
    "    #     return tuple(zip(*batch))\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def collate_fn(batch):\n",
    "    #     images = []\n",
    "    #     targets = []\n",
    "    #     for img, target in batch:\n",
    "    #         images.append(ToTensor()(img))\n",
    "    #         targets.append(target)\n",
    "    #     return images, targets\n",
    "\n",
    "\n",
    "\n",
    "# # check dataset\n",
    "# dataset = SVED_Dataset(\"datasets/iamges/train\", transforms=SVED_Compose([SVED_ToTensor(), SVED_RandomHorizontalFlip(.5)]))\n",
    "# print('length of dataset = ', len(dataset), '\\n')\n",
    "#\n",
    "# # getting the image and target for a test index.  Feel free to change the index.\n",
    "# img, targets = dataset[0]\n",
    "# print(img.shape, '\\n',targets)\n",
    "# img, target = dataset[0]\n",
    "# print(img.shape, '\\n',targets)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for img, target in batch:\n",
    "        images.append(img)\n",
    "        targets.append(target)\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a0631",
   "metadata": {},
   "source": [
    "# Training Faster-RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345af391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 dataloader workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sullivan/anaconda3/envs/docExtractor2/lib/python3.6/site-packages/torch/cuda/__init__.py:120: UserWarning: \n",
      "    Found GPU%d %s which is of cuda capability %d.%d.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is %d.%d.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn.format(d, name, major, minor, min_arch // 10, min_arch % 10))\n",
      "/home/sullivan/anaconda3/envs/docExtractor2/lib/python3.6/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [0/5]  eta: 0:00:02  lr: 0.001254  loss: 1.8787 (1.8787)  loss_classifier: 1.5545 (1.5545)  loss_box_reg: 0.0019 (0.0019)  loss_objectness: 0.3124 (0.3124)  loss_rpn_box_reg: 0.0099 (0.0099)  time: 0.5242  data: 0.2002  max mem: 1408\n",
      "Epoch: [0]  [4/5]  eta: 0:00:00  lr: 0.005000  loss: 0.8966 (1.1626)  loss_classifier: 0.2603 (0.7042)  loss_box_reg: 0.0186 (0.0599)  loss_objectness: 0.3124 (0.3780)  loss_rpn_box_reg: 0.0115 (0.0205)  time: 0.5715  data: 0.2523  max mem: 1792\n",
      "Epoch: [0] Total time: 0:00:02 (0.5719 s / it)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-33e5378a7546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# evaluate on the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mtrain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bbox'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;31m# write the training results to tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrappel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/docExtractor2/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/sullivan/Projet-Recherche/engine.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mcoco_evaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCocoEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/sullivan/Projet-Recherche/utils.py\u001b[0m in \u001b[0;36mlog_every\u001b[0;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    170\u001b[0m         \u001b[0mMB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/docExtractor2/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/docExtractor2/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/docExtractor2/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/docExtractor2/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-43e256fd6b3c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# reading the images and converting them to correct size and color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# width and heights parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define the parameters for training\n",
    "num_epochs = 5000\n",
    "batch_size = 1\n",
    "learning_rate = 5e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005 \n",
    "\n",
    "# learning rate schedule\n",
    "lr_gamma = 0.33\n",
    "lr_dec_step_size = 20\n",
    "\n",
    "data_transform = {\n",
    "        \"train\": SVED_Compose([SVED_ToTensor(), SVED_RandomHorizontalFlip(.12)]),\n",
    "        \"val\": SVED_Compose([SVED_ToTensor()]),\n",
    "        \"test\": SVED_Compose([SVED_ToTensor()])\n",
    "    }\n",
    "\n",
    "\n",
    "# create the train dataset and dataloader\n",
    "train_dataset = SVED_Dataset(\"datasets/images/train\", transforms=data_transform[\"train\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw, collate_fn=collate_fn)\n",
    "\n",
    "# create the val dataset and dataloader\n",
    "val_dataset = SVED_Dataset(\"datasets/images/val\", transforms=data_transform[\"val\"])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=nw, collate_fn=collate_fn)\n",
    "\n",
    "# create the val dataset and dataloader\n",
    "test_dataset = SVED_Dataset(\"datasets/images/test\", transforms=data_transform[\"test\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=nw, collate_fn=collate_fn)\n",
    "\n",
    "# load a pre-trained Faster R-CNN model from the torchvision model zoo\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# replace the pre-trained classifier with a new one that has the correct number of classes\n",
    "num_classes = 4 + 1  # replace with the number of classes in your dataset\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_dec_step_size, gamma= lr_gamma)\n",
    "\n",
    "\n",
    "# train the model for the specified number of epochs\n",
    "writer = SummaryWriter()\n",
    "for epoch in range(num_epochs):\n",
    "    # training for one epoch\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=500)\n",
    "    # write the train loss to tensorboard\n",
    "    writer.add_scalar('train_lr', train_loss.meters['lr'].value, epoch)\n",
    "    writer.add_scalar('train_loss', train_loss.meters['loss'].value, epoch)\n",
    "    writer.add_scalar('train_loss_classifier', train_loss.meters['loss_classifier'].value, epoch)\n",
    "    writer.add_scalar('train_loss_box_reg', train_loss.meters['loss_box_reg'].value, epoch)\n",
    "    writer.add_scalar('train_loss_objectness', train_loss.meters['loss_objectness'].value, epoch)\n",
    "    writer.add_scalar('train_loss_rpn_box_reg', train_loss.meters['loss_rpn_box_reg'].value, epoch)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the training dataset\n",
    "    train_output = evaluate(model, train_loader, device=device).coco_eval['bbox'].stats\n",
    "    # write the training results to tensorboard\n",
    "    precision, rappel = train_output[:6], train_output[6:]\n",
    "    writer.add_scalar('train_precision', np.mean(precision[precision >= 0]), epoch)\n",
    "    writer.add_scalar('train_rappel', np.mean(rappel[rappel >= 0]), epoch)\n",
    "    # evaluate on the validation dataset\n",
    "    val_output = evaluate(model, val_loader, device=device).coco_eval['bbox'].stats\n",
    "    # write the val results to tensorboard\n",
    "    precision, rappel = val_output[:6], val_output[6:]\n",
    "    writer.add_scalar('val_precision', np.mean(precision[precision >= 0]), epoch)\n",
    "    writer.add_scalar('val_rappel', np.mean(rappel[rappel >= 0]), epoch)\n",
    "\n",
    "test_output = evaluate(model, test_loader, device=device)['bbox'].stats\n",
    "precision, rappel = test_output[:6], test_output[6:]\n",
    "writer.add_scalar('test_precision', np.mean(precision[precision >= 0]), epoch)\n",
    "writer.add_scalar('test_rappel', np.mean(rappel[rappel >= 0]), epoch)\n",
    "print(\"Training completed!\")\n",
    "torch.save(model, 'my_weights/faster_rcnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_faster_rcnn(image_path):\n",
    "    categories = ['_', 'Initial', 'Decoration', 'ContentIllustration', 'PrintersMark']\n",
    "    #image loading\n",
    "    model.eval()\n",
    "    transform = ToTensor()\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except:\n",
    "        urllib.request.urlretrieve(image_path, \"image.jpg\")\n",
    "        image = Image.open(\"image.jpg\")\n",
    "\n",
    "    #prediction\n",
    "    tensor_image = transform(image).unsqueeze(0).to(device)\n",
    "    prediction = model(tensor_image)\n",
    "\n",
    "    #list of boxes and labels\n",
    "    boxes = prediction[0]['boxes']\n",
    "    labels = prediction[0]['labels']\n",
    "    scores = prediction[0]['scores']\n",
    "    keep = torchvision.ops.nms(boxes, scores, .1)\n",
    "    print(boxes, labels)\n",
    "\n",
    "    for (box, label) in zip(boxes[keep], labels[keep]):\n",
    "        rectangle_box = [(box[0], box[1]), (box[2], box[3])]\n",
    "        rectangle_label = categories[label.item()]\n",
    "\n",
    "        img_box = ImageDraw.Draw(image)\n",
    "        img_box.rectangle(rectangle_box, outline=\"red\")\n",
    "        img_box.text(rectangle_box[1], text=rectangle_label, fill=\"red\")\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d340ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_faster_rcnn('datasets/images/train/1015.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4fa57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
